{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique:LoRa \n",
    "* Model: distilgpt2\n",
    "* Evaluation approach: Trainer-based evaluation.\n",
    "* Fine-tuning dataset: SST-2 (Stanford Sentiment Treebank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset.\n",
    "\n",
    "**Some code was reused from the lab of the workpaces of the course.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa15fdc6d574487f96864386b9f356cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/35.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 3.11M/3.11M [00:00<00:00, 9.48MB/s]\n",
      "Downloading data: 100%|██████████| 72.8k/72.8k [00:00<00:00, 463kB/s]\n",
      "Downloading data: 100%|██████████| 148k/148k [00:00<00:00, 971kB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af018b99272e4af6a4bc522589bb9eb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/67349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b0baea012c0471fb909d92efac12988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da2c2ae661f04c34b6cf982107b068d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "import torch\n",
    "print(torch.cuda.is_available())  \n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "dataset\n",
    "sample_size = 5000\n",
    "random_indices = random.sample(range(len(dataset['train'])), sample_size)\n",
    "dataset['train'] = dataset['train'].select(random_indices)\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4df681",
   "metadata": {},
   "source": [
    "**Identify if the classes are imbalanced**\n",
    "\n",
    "This training set is imbalanced but not extremly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8a266b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Negative Class samples (label 0): 2216\n",
      "Number of Positive Class samples (label 1): 2784\n"
     ]
    }
   ],
   "source": [
    "train_labels = dataset['train']['label']\n",
    "\n",
    "negative_count = sum(label == 0 for label in train_labels)  # Label 0 represents negative sentiment\n",
    "positive_count = sum(label == 1 for label in train_labels)  # Label 1 represents positive sentiment\n",
    "\n",
    "print(f\"Number of Negative Class samples (label 0): {negative_count}\")\n",
    "print(f\"Number of Positive Class samples (label 1): {positive_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b4818952d6546b9ba12bff5d25b28be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "782551ddfa024d0aa24ceed574b6faae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d8a62d248a54bc281b813136a12f605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c773a10263924b289f5c4b6f56649270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9504c99355754ff289e39d24270b651a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ce1baafaa0346d795b9b3ec978fe276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c971c603d94e4885baa58a8db247f587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/872 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d3165351634a57acb4f7e69a2c0ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1821 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenized_ds = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_ds[\"train\"] = tokenized_ds[\"train\"].shuffle(seed=21)\n",
    "tokenized_ds[\"validation\"] = tokenized_ds[\"validation\"].shuffle(seed=21)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7fbfe64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['negative', 'positive'], id=None),\n",
       " 'idx': Value(dtype='int32', id=None),\n",
       " 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds[\"train\"].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc5722d956c438981791ac703f7ee6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilgpt2\",\n",
    "    num_labels=2,\n",
    "    id2label={0: \"negative\", 1: \"positive\"},\n",
    "    label2id={\"negative\": 0, \"positive\": 1},\n",
    ")\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68f7cc8",
   "metadata": {},
   "source": [
    "**How to try Imbalanced Classes**\n",
    "To try imabalanced classes, I used this article \n",
    "https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/\n",
    "\n",
    "For me, false negatives are particularly costly in this case. Why? If the model incorrectly classifies an entry as NEGATIVE when it's not, the company would likely implement retention strategies to try and change the customer's opinion, which would incur additional expenses. Given this context,I will calculate   F2 score whic emphasizes minimizing false negatives by weighing recall twice as much as precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d7e69fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.2)\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.8/301.8 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.19.5 in /home/student/.local/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.2 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='55' max='55' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [55/55 00:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 6.920271396636963,\n",
       " 'eval_f2-score': 0.8383685800604229,\n",
       " 'eval_runtime': 21.5974,\n",
       " 'eval_samples_per_second': 40.375,\n",
       " 'eval_steps_per_second': 2.547}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, recall_score, precision_score, f1_score,fbeta_score\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    f2=fbeta_score(labels, predictions, beta=2)\n",
    "    return {\"f2-score\": f2}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./polarity\",\n",
    "        per_device_eval_batch_size=16,  \n",
    "        fp16=True,  \n",
    "        logging_steps=100),\n",
    "    eval_dataset=tokenized_ds['validation'],  \n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights.\n",
    "\n",
    "**Notice: I used the following reference to build  LORAConfig. https://github.com/huggingface/blog/blob/main/Lora-for-sequence-classification-with-Roberta-Llama-Mistral.md**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.10/site-packages/peft/tuners/lora.py:475: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: base_model.model.transformer.wte.weight, Trainable: False\n",
      "Parameter: base_model.model.transformer.wpe.weight, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.0.ln_1.weight, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.0.ln_1.bias, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.0.attn.c_attn.weight, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.0.attn.c_attn.bias, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.0.attn.c_attn.lora_A.default.weight, Trainable: True\n",
      "Parameter: base_model.model.transformer.h.0.attn.c_attn.lora_B.default.weight, Trainable: True\n",
      "Parameter: base_model.model.transformer.h.0.attn.c_proj.weight, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.0.attn.c_proj.bias, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.0.ln_2.weight, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.0.ln_2.bias, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.0.mlp.c_fc.weight, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.0.mlp.c_fc.bias, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.0.mlp.c_proj.weight, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.0.mlp.c_proj.bias, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.1.ln_1.weight, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.1.ln_1.bias, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.1.attn.c_attn.weight, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.1.attn.c_attn.bias, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.1.attn.c_attn.lora_A.default.weight, Trainable: True\n",
      "Parameter: base_model.model.transformer.h.1.attn.c_attn.lora_B.default.weight, Trainable: True\n",
      "Parameter: base_model.model.transformer.h.1.attn.c_proj.weight, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.1.attn.c_proj.bias, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.1.ln_2.weight, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.1.ln_2.bias, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.1.mlp.c_fc.weight, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.1.mlp.c_fc.bias, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.1.mlp.c_proj.weight, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.1.mlp.c_proj.bias, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.2.ln_1.weight, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.2.ln_1.bias, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.2.attn.c_attn.weight, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.2.attn.c_attn.bias, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.2.attn.c_attn.lora_A.default.weight, Trainable: True\n",
      "Parameter: base_model.model.transformer.h.2.attn.c_attn.lora_B.default.weight, Trainable: True\n",
      "Parameter: base_model.model.transformer.h.2.attn.c_proj.weight, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.2.attn.c_proj.bias, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.2.ln_2.weight, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.2.ln_2.bias, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.2.mlp.c_fc.weight, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.2.mlp.c_fc.bias, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.2.mlp.c_proj.weight, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.2.mlp.c_proj.bias, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.3.ln_1.weight, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.3.ln_1.bias, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.3.attn.c_attn.weight, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.3.attn.c_attn.bias, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.3.attn.c_attn.lora_A.default.weight, Trainable: True\n",
      "Parameter: base_model.model.transformer.h.3.attn.c_attn.lora_B.default.weight, Trainable: True\n",
      "Parameter: base_model.model.transformer.h.3.attn.c_proj.weight, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.3.attn.c_proj.bias, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.3.ln_2.weight, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.3.ln_2.bias, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.3.mlp.c_fc.weight, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.3.mlp.c_fc.bias, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.3.mlp.c_proj.weight, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.3.mlp.c_proj.bias, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.4.ln_1.weight, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.4.ln_1.bias, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.4.attn.c_attn.weight, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.4.attn.c_attn.bias, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.4.attn.c_attn.lora_A.default.weight, Trainable: True\n",
      "Parameter: base_model.model.transformer.h.4.attn.c_attn.lora_B.default.weight, Trainable: True\n",
      "Parameter: base_model.model.transformer.h.4.attn.c_proj.weight, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.4.attn.c_proj.bias, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.4.ln_2.weight, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.4.ln_2.bias, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.4.mlp.c_fc.weight, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.4.mlp.c_fc.bias, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.4.mlp.c_proj.weight, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.4.mlp.c_proj.bias, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.5.ln_1.weight, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.5.ln_1.bias, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.5.attn.c_attn.weight, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.5.attn.c_attn.bias, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.5.attn.c_attn.lora_A.default.weight, Trainable: True\n",
      "Parameter: base_model.model.transformer.h.5.attn.c_attn.lora_B.default.weight, Trainable: True\n",
      "Parameter: base_model.model.transformer.h.5.attn.c_proj.weight, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.5.attn.c_proj.bias, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.5.ln_2.weight, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.5.ln_2.bias, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.5.mlp.c_fc.weight, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.5.mlp.c_fc.bias, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.5.mlp.c_proj.weight, Trainable: False\n",
      "Parameter: base_model.model.transformer.h.5.mlp.c_proj.bias, Trainable: False\n",
      "Parameter: base_model.model.transformer.ln_f.weight, Trainable: False\n",
      "Parameter: base_model.model.transformer.ln_f.bias, Trainable: False\n",
      "Parameter: base_model.model.score.original_module.weight, Trainable: True\n",
      "Parameter: base_model.model.score.modules_to_save.default.weight, Trainable: True\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model,TaskType\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=8, \n",
    "    lora_alpha=32,  \n",
    "    bias='none',\n",
    "    lora_dropout=0.1)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilgpt2\",\n",
    "    num_labels=2,\n",
    "    id2label={0: \"negative\", 1: \"positive\"},\n",
    "    label2id={\"negative\": 0, \"positive\": 1},\n",
    ")\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "for name, param in lora_model.named_parameters():\n",
    "    print(f\"Parameter: {name}, Trainable: {param.requires_grad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "894046c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1875/1875 18:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F2-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.407800</td>\n",
       "      <td>0.419026</td>\n",
       "      <td>0.776902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.391000</td>\n",
       "      <td>0.384073</td>\n",
       "      <td>0.864143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.321500</td>\n",
       "      <td>0.372836</td>\n",
       "      <td>0.864695</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./polarity/checkpoint-625 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='109' max='109' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [109/109 00:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.37283584475517273,\n",
       " 'eval_f2-score': 0.8646953405017921,\n",
       " 'eval_runtime': 23.9148,\n",
       " 'eval_samples_per_second': 36.463,\n",
       " 'eval_steps_per_second': 4.558,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    f2=fbeta_score(labels, predictions, beta=2)\n",
    "    return {\"f2-score\": f2}\n",
    "\n",
    "torch.cuda.empty_cache() \n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./polarity\",\n",
    "        learning_rate=2e-4,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=100,\n",
    "        fp16=False,\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.save_pretrained(\"distilgpt2-lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f807503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce39808b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47abf88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='109' max='109' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [109/109 00:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.37283584475517273, 'eval_f2-score': 0.8646953405017921, 'eval_runtime': 23.2993, 'eval_samples_per_second': 37.426, 'eval_steps_per_second': 4.678}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from peft import PeftModel\n",
    "from peft import AutoPeftModelForSequenceClassification\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    f2=fbeta_score(labels, predictions, beta=2)\n",
    "    return {\"f2-score\": f2}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "lora_model = AutoPeftModelForSequenceClassification.from_pretrained(\"distilgpt2-lora\", num_labels=2)\n",
    "lora_model.resize_token_embeddings(len(tokenizer))\n",
    "lora_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "trainer_lora_model = Trainer(\n",
    "    model=lora_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./polarity\",\n",
    "        per_device_eval_batch_size=8,  \n",
    "        fp16=True,  \n",
    "        logging_steps=100),\n",
    "    eval_dataset=tokenized_ds['validation'],  \n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "with torch.no_grad():  \n",
    "    print(trainer_lora_model.evaluate())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
